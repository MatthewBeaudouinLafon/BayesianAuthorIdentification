{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Author Identification\n",
    "#### By Carl Moser and Matthew Beaudouin-Lafon\n",
    "\n",
    "Our goal for this project is to correctly identify the author of a text from a finite number of known authors.\n",
    "To accomplish this, we process text from known authors and use this information to generate a likelihood function, which is used to perform a Bayesian update. In our first iteration, we simply applied word frequency analysis to the known texts. In the second iteration, we built Markov Chains for each author to capture sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from thinkbayes2 import Suite\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "import matplotlib.pyplot as plt\n",
    "import thinkplot\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def pickleDump(filename, todump):\n",
    "    out = open('DATs/' + filename, 'wb+')\n",
    "    for d in todump:\n",
    "        dump(d, out)\n",
    "    out.close()\n",
    "    \n",
    "def pickleLoad(fileName):\n",
    "    f = open('DATs/' + fileName + '.dat', 'rb')\n",
    "    testText = load(f)\n",
    "    f.close()\n",
    "    return testText\n",
    "    \n",
    "def getPickledMarkov(fileName):\n",
    "    infile = open('DATs/' + fileName, 'rb+')\n",
    "    chain = load(infile)\n",
    "    wordCount = load(infile)\n",
    "    author = load(infile)\n",
    "    infile.close()\n",
    "    return (chain, wordCount, author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we process text files to generate Markov Chains and Word Frequency dictionaries. These data structures are then stored in files using Pickle. You can get the text (and DAT) files that we used [in our repository](https://github.com/MatthewBeaudouinLafon/BayesianAuthorIdentification).\n",
    "We strip all punctuation and case sensitivity such that we only look at pairs of words. We made the decision to seperate and compute sentences seperately since we don't think that the last word of one sentence is related to the first word of the following sentence in the context of Markov Chains. This assumption may not be as accurate for text like poetry.\n",
    "In the Markov Chain, the beginning of a sentence is denoted as '*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MarkovChain():\n",
    "    \"\"\"\n",
    "    Markov Chain class. \n",
    "    chain:       nested dictionary representing the number of occurences of a word given the previous word.\n",
    "    wordCount:   dictionary of the number of total number of words (value) that have occured after the previous word (key).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, author):\n",
    "        \"\"\"\n",
    "        Parameter: \n",
    "            - author: name of author (string)\n",
    "        \"\"\"\n",
    "        self.author = author\n",
    "        self.chain = {'*': {}}\n",
    "        self.wordCount = {'*': 0}\n",
    "    \n",
    "    def addWord(self, prevWord, word):\n",
    "        \"\"\"\n",
    "        Add a word to the Markov Chain.\n",
    "        Takes the previous word and the current word as strings\n",
    "        \"\"\"\n",
    "        self.chain[prevWord][word] = 1 + self.chain[prevWord].get(word, 0)\n",
    "        self.wordCount[prevWord]= 1 + self.wordCount.get(prevWord, 0)\n",
    "    \n",
    "        # When encountering a new word, add it to the prefix dictionary\n",
    "        if not self.chain.get(word):\n",
    "            self.chain[word] = {}\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Process a \"sentence\" to produce a Markov Chain. Takes a sentence as a list of lowercase words.\n",
    "        NO ASTERISKS. SERIOUSLY.\n",
    "        \"\"\"\n",
    "        # '*' represents the beginning of a sentence in the chain. \n",
    "        # This way, we can determine the probability of a word starting a sentence\n",
    "        sentence = ['*'] + sentence\n",
    "        if len(sentence) > 1:\n",
    "            for i in [i + 1 for i in range(len(sentence)-1)]: # Start at the second word\n",
    "                self.addWord(sentence[i-1], sentence[i])\n",
    "        \n",
    "    def getProb(self, prevWord, word):\n",
    "        \"\"\"\n",
    "        Return the probability of getting word given prevWord.\n",
    "        Takes two strings.\n",
    "        \"\"\"\n",
    "        return self.chain[prevWord][word]/self.wordCount[prevWord]\n",
    "    \n",
    "def processGutenberg(fileName, author, make=True):\n",
    "    \"\"\"\n",
    "    Process a Gutenberg text file.\n",
    "    fileName: string\n",
    "    author: string\n",
    "    returns a markovChain object.\n",
    "    \"\"\"\n",
    "    f = open('Texts/' + fileName + '.txt')\n",
    "    \n",
    "    #Skip to the beginning of the actual text\n",
    "    for line in f:\n",
    "        if line.startswith(\"*** START OF THIS PROJECT\"):\n",
    "            break\n",
    "    \n",
    "    text = ''\n",
    "    \n",
    "    # Put the text into one big string\n",
    "    for line in f:\n",
    "        # Stop when hitting the end of the book\n",
    "        if line.startswith(\"*** END OF THIS PROJECT\"):\n",
    "            break\n",
    "        \n",
    "        text += line + ' '\n",
    "        \n",
    "    sentences = re.split('[.?!]', text) # Seperate text into a list of sentences sentence\n",
    "    \n",
    "    listOSentences = []\n",
    "    for sentence in sentences:\n",
    "        # Make all words lowercase and strip off punctuation\n",
    "        sentenceList = ''.join(char for char in sentence if char in set(string.ascii_letters + string.digits + ' ')).lower().split()\n",
    "        \n",
    "        if sentenceList != []:\n",
    "            listOSentences.append(sentenceList)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    if make==True:\n",
    "        m = makeMarkov(listOSentences, author)\n",
    "        pickleDump(fileName + '.dat', [m.chain, m.wordCount, m.author])\n",
    "    else:\n",
    "        pickleDump(fileName + '.dat', [listOSentences, author])\n",
    "\n",
    "def makeMarkov(sentenceList, author):\n",
    "    markovChain = MarkovChain(author)\n",
    "    for sentence in sentenceList:\n",
    "        # Process the sentence in the Markov Chain\n",
    "        markovChain.addSentence(sentence)\n",
    "    return markovChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Books are stored as text files in the format 'bookName.txt'\n",
    "books = [('GreatExpectations', 'Charles Dickens'), ('Frankenstein', 'Mary Shelley'),\n",
    "         ('RomeoAndJuliet', 'Shakespeare'), ('MobyDick', 'Herman Melville'), ('Twilight', 'Stephenie Meyer'),\n",
    "         (\"The Hitch Hiker's Guide to the Galaxy\", 'Douglas Adams')]\n",
    "\n",
    "unknown = ['whoWroteFrankenstein', 'whoWroteNewMoon', 'whoWroteWikipediaArticle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate DAT files for text from known authors\n",
    "for book in books:\n",
    "    name, author = book\n",
    "    processGutenberg(name, author)\n",
    "    \n",
    "# Generate DAT files for other texts\n",
    "for text in unknown:\n",
    "    processGutenberg(text, 'Unknown', make=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Based Identification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to look at the word frequency of a text written by each of the authors that we chose. For the class, we made the author's name be the hypothesis and ddecided to store the word counts for the authors in a dicitonary. The word counts themselves are also dictionaries with the words as the key and the number of times that they appear in the text as the value. The Likelihood function takes in a word as the data and the author as the hypothesis. We use the author to find the word count within the class dictionary. From there, we return the number of times the word showed up in the author's text. Since not all of the words that we will be updating with will be in all of the different authors' word count, we decided to make a function within the class that checks if a word is in all of the authors' word count. With this information, we can decide whether or not to run Update with a given word in the sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FreqId(Suite):\n",
    "    \"\"\"\n",
    "    Bayesian model for author identification. Uses Markov chains generated from texts who's authors are known as a likelihood function.\n",
    "    Has a list of {author : MarkovChain objects}\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.markovChains = {}\n",
    "        self.totalWords = 0\n",
    "        Suite.__init__(self)\n",
    "    \n",
    "    def isWorthChecking(self, word):\n",
    "        worth = True\n",
    "        for author, prob in self.Items():\n",
    "            wordCount = self.markovChains[author][1]\n",
    "            worth = worth and bool(wordCount.get(word))\n",
    "        return worth\n",
    "                \n",
    "    def Likelihood(self, data, hypo):\n",
    "        \"\"\"\n",
    "        data: word as string\n",
    "        hypo: author as string\n",
    "        \"\"\"\n",
    "        hypoAuthor = hypo\n",
    "        chain, wordCount = self.markovChains[hypoAuthor]\n",
    "        word = data\n",
    "        #print('prob of ' + hypoAuthor +' = ', wordCount[word]/float(self.totalWords))\n",
    "        return wordCount[word]/float(self.totalWords)\n",
    "    \n",
    "def runFreqIdentification(testText, freqId):\n",
    "    # Reset the Suite\n",
    "    author_probs = {}\n",
    "    for author, prob in freqId.Items():\n",
    "        author_probs[author] = [prob]\n",
    "        freqId[author] = 1\n",
    "\n",
    "    freqId.Normalize()\n",
    "    \n",
    "    # Run Updates\n",
    "    for sentence in testText:\n",
    "        sentence = ['*'] + sentence  # * represents the beginning of a sentence in the Markov Chain\n",
    "        for word in sentence:\n",
    "            if freqId.isWorthChecking(word):\n",
    "                freqId.Update(word)\n",
    "                for author, prob in freqId.Items():\n",
    "                    author_probs[author].append(prob)\n",
    "                    #if prob != 1:\n",
    "                        #print(author, prob)\n",
    "                \n",
    "    for author in author_probs:\n",
    "        x = range(0, len(author_probs[author]))\n",
    "        plt.plot(x, author_probs[author], label=author)\n",
    "        \n",
    "    plt.suptitle('Frequency Identification')\n",
    "    plt.xlabel('Update')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=2)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "                \n",
    "    # Print the authors and their likelihoods nicely\n",
    "    for key, value in freqId.Items():\n",
    "        print(key, value)\n",
    "    print # Line return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Based Identification\n",
    "\n",
    "Similarly to what we did above, MarkovId inherits from thinbayes' Suite class to keep track of the Bayesian model. As such, it overloads the Likelihood method from Suite. The idea is to walk through the unknown text two words at a time. The goal of the likelihood function is to evaluate the probability that a certain author has written the text (based off their generated Markov Chain). It's important to note that this probability will be small even if the for the actual author of the text, but it will relatively be larger than the likelihood for the other authors. Bayesian inference allows us to normalize through.\n",
    "We could get the likelihood by walking through the chain and multiplying the probabilities. However:\n",
    " * Since all probabilities are smaller than 1, is isn't unfathomable that we could hit the minimum float\n",
    " * We still have the same issue as previously of querying words that other authors haven't used  \n",
    " \n",
    "The first issue is fairly minor; we could intelligently multiply by a constant factor that would get normalized for during the Bayesian update. For the second problem, we can't just pop off unwanted words from the sentence, otherwise we could make false connections. If we had a sequence 'a b c', and only 'a' and 'c' have been used by all authors, popping 'b' would make a Markov connection between 'a' and 'c'. So instead of just walking through and multiplying probabilities, we do individual updates on pairs of words, at the cost of computation time. When we encounter a word not seen before, we just skip that update. We do lose some information in the process since seeing a word that isn't in an author's vocabulary is strong evidence that they did not write the piece. We reconcile that by acknowledging that we can compensate with larger unknown text. The other solution would have been to assign to an unknown pair the probability of the least likely pair of words. We are less comfortable with it because instead of discarding information we are essentially fabricating. We cannot establish how much less likely it is for one author to be using a word over another if they have never used it.\n",
    "\n",
    "So in practice, we have an 'isWorthChecking' method that checks the chain for both words in the Markov Chain before running the Update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MarkovId(Suite):\n",
    "    \"\"\"\n",
    "    Bayesian model for author identification. Uses Markov chains generated from texts who's authors are known as a likelihood function.\n",
    "    Has a list of {author : MarkovChain objects}\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.markovChains = {}\n",
    "        Suite.__init__(self)\n",
    "                \n",
    "    def Likelihood(self, data, hypo):\n",
    "        \"\"\"\n",
    "        data: (string, string)\n",
    "        hypo: (Markov Chain, Word Count dictionary, Author)\n",
    "        \"\"\"\n",
    "        hypoAuthor = hypo\n",
    "        chain, wordCount = self.markovChains[hypoAuthor]\n",
    "        prevWord, word = data\n",
    "        \n",
    "        try:\n",
    "            like = chain[prevWord][word]/float(wordCount[prevWord])\n",
    "        except:\n",
    "            print('isWorthChecking failed to check this pair of words')\n",
    "            raise\n",
    "            \n",
    "        return like\n",
    "    \n",
    "    def isWorthChecking(self, prevWord, word):\n",
    "        \"\"\"\n",
    "        Returns boolean based on whether a word is useful for bayesian update.\n",
    "        A word is useful if it is found in the Markov Chain. \n",
    "        \"\"\"\n",
    "        worth = True\n",
    "        for author, prob in self.Items():                   # Check every author\n",
    "            chain = self.markovChains[author][0]\n",
    "            worth = worth and (bool(chain.get(prevWord))        # Check the chain has prevWord\n",
    "                          and bool(chain[prevWord].get(word)))  # And the sequence (prevWord, word)\n",
    "\n",
    "        return worth\n",
    "    \n",
    "def runMarkovIdentification(testText, markovId):\n",
    "    # Reset the Suite\n",
    "    author_probs = {}\n",
    "    for author, prob in markovId.Items():\n",
    "        author_probs[author] = [prob]\n",
    "        markovId[author] = 1\n",
    "\n",
    "    markovId.Normalize()\n",
    "    for sentence in testText:\n",
    "        sentence = ['*'] + sentence # * represents the beginning of a sentence in the Markov Chain\n",
    "\n",
    "        for i in range(len(sentence) - 1):          \n",
    "            if markovId.isWorthChecking(sentence[i], sentence[i + 1]): # Check that the words are worth checking\n",
    "                markovId.Update((sentence[i], sentence[i + 1]))        # Perform the Update\n",
    "                for author, prob in markovId.Items():\n",
    "                    author_probs[author].append(prob)\n",
    "                \n",
    "    for author in author_probs:\n",
    "        x = range(0, len(author_probs[author]))\n",
    "        plt.plot(x, author_probs[author], label=author)\n",
    "        \n",
    "    plt.suptitle('Markov Identification')\n",
    "    plt.xlabel('Update')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "           fancybox=True, shadow=True, ncol=2)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    # Print the authors and their likelihoods nicely\n",
    "    for key, value in markovId.Items():\n",
    "        print(key, value)\n",
    "    print # Line return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = ['Frankenstein.dat', 'GreatExpectations.dat', 'RomeoAndJuliet.dat', 'MobyDick.dat', \"The Hitch Hiker's Guide to the Galaxy.dat\", 'Twilight.dat']\n",
    "\n",
    "markovId = MarkovId()\n",
    "freqId = FreqId()\n",
    "\n",
    "# Retrieve all pickled Markov Chains\n",
    "for f in files:\n",
    "    chain, wordCount, author = getPickledMarkov(f)\n",
    "    freqId[author] = 1\n",
    "    markovId[author] = 1\n",
    "    freqId.markovChains[author] = (chain, wordCount)\n",
    "    freqId.totalWords = sum(wordCount.values())\n",
    "    markovId.markovChains[author] = (chain, wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-252dc0bbf484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickleLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'whoWroteFrankenstein'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrunFreqIdentification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrunMarkovIdentification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarkovId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-dcf006883a1e>\u001b[0m in \u001b[0;36mrunFreqIdentification\u001b[0;34m(testText, freqId)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mauthor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mauthor_probs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthor_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauthor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frequency Identification'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carl/anaconda3/envs/py3k/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carl/anaconda3/envs/py3k/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carl/anaconda3/envs/py3k/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carl/anaconda3/envs/py3k/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carl/anaconda3/envs/py3k/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carl/anaconda3/envs/py3k/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must have same first dimension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    }
   ],
   "source": [
    "text = pickleLoad('whoWroteFrankenstein')\n",
    "runFreqIdentification(text, freqId)\n",
    "runMarkovIdentification(text, markovId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = pickleLoad('whoWroteNewMoon')\n",
    "runFreqIdentification(text, freqId)\n",
    "runMarkovIdentification(text, markovId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = pickleLoad('whoWroteWikipediaArticle')\n",
    "runFreqIdentification(text, freqId)\n",
    "runMarkovIdentification(text, markovId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
