{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Generation\n",
    "In this notebook, we generate the necessary Markov Chains for each author which will be used as likelihood functions during the identification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import sys\n",
    "import re\n",
    "\n",
    "class MarkovChain():\n",
    "    \"\"\"\n",
    "    Markov Chain class. \n",
    "    chain:       nested dictionary representing the number of occurences of a word given the previous word.\n",
    "    wordCount:   dictionary of the number of total number of words (value) that have occured after the previous word (key).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, author):\n",
    "        \"\"\"\n",
    "        Parameter: \n",
    "            - author: name of author (string)\n",
    "        \"\"\"\n",
    "        self.author = author\n",
    "        self.chain = {'*': {}}\n",
    "        self.wordCount = {'*': 0}\n",
    "    \n",
    "    def addWord(self, prevWord, word):\n",
    "        \"\"\"\n",
    "        Add a word to the Markov Chain.\n",
    "        Takes the previous word and the current word as strings\n",
    "        \"\"\"\n",
    "        self.chain[prevWord][word] = 1 + self.chain[prevWord].get(word, 0)\n",
    "        self.wordCount[prevWord]= 1 + self.wordCount.get(prevWord, 0)\n",
    "    \n",
    "        # When encountering a new word, add it to the prefix dictionary\n",
    "        if not self.chain.get(word):\n",
    "            self.chain[word] = {}\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Process a \"sentence\" to produce a Markov Chain. Takes a sentence as a list of lowercase words.\n",
    "        NO ASTERISKS. SERIOUSLY.\n",
    "        \"\"\"\n",
    "        # '*' represents the beginning of a sentence in the chain. \n",
    "        # This way, we can determine the probability of a word starting a sentence\n",
    "        sentence = ['*'] + sentence\n",
    "        if len(sentence) > 1:\n",
    "            for i in [i + 1 for i in range(len(sentence)-1)]: # Start at the second word\n",
    "                self.addWord(sentence[i-1], sentence[i])\n",
    "        \n",
    "    def getProb(self, prevWord, word):\n",
    "        \"\"\"\n",
    "        Return the probability of getting word given prevWord.\n",
    "        Takes two strings.\n",
    "        \"\"\"\n",
    "        return self.chain[prevWord][word]/self.wordCount[prevWord]\n",
    "    \n",
    "def processGutenberg(fileName, author, make=True):\n",
    "    \"\"\"\n",
    "    Process a Gutenberg text file.\n",
    "    fileName: string\n",
    "    author: string\n",
    "    returns a markovChain object.\n",
    "    \"\"\"\n",
    "    f = open(fileName)\n",
    "    \n",
    "    #Skip to the beginning of the actual text\n",
    "    for line in f:\n",
    "        if line.startswith(\"*** START OF THIS PROJECT\"):\n",
    "            break\n",
    "    \n",
    "    text = ''\n",
    "    \n",
    "    # Put the text into one big string\n",
    "    for line in f:\n",
    "        # Stop when hitting the end of the book\n",
    "        if line.startswith(\"*** END OF THIS PROJECT\"):\n",
    "            break\n",
    "        \n",
    "        text += line + ' '\n",
    "        \n",
    "    sentences = re.split('[.?!]', text) # Seperate text into a list of sentences sentence\n",
    "    \n",
    "    listOSentences = []\n",
    "    for sentence in sentences:\n",
    "        # Make all words lowercase and strip off punctuation\n",
    "        sentenceList = ''.join(char for char in sentence if char in set(string.letters + string.digits + ' ')).lower().split()\n",
    "        \n",
    "        if sentenceList != []:\n",
    "            listOSentences.append(sentenceList)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    if make==True:\n",
    "        return makeMarkov(listOSentences, author)\n",
    "    else:\n",
    "        return listOSentences\n",
    "\n",
    "def makeMarkov(sentenceList, author):\n",
    "    markovChain = MarkovChain(author)\n",
    "    for sentence in sentenceList:\n",
    "        # Process the sentence in the Markov Chain\n",
    "        markovChain.addSentence(sentence)\n",
    "    return markovChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "greatExp = processGutenberg('GreatExpectations.txt', 'Charles Dickens')\n",
    "frank = processGutenberg('Frankenstein.txt', 'Mary Shelley')\n",
    "romeoJuliet = processGutenberg('RomeoAndJuliet.txt', 'Shakespeare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['o', 'that', 'this', 'too', 'too', 'solid', 'flesh', 'would', 'melt', 'thaw', 'and', 'resolve', 'itself', 'into', 'a', 'dew'], ['or', 'that', 'the', 'everlasting', 'had', 'not', 'fixd', 'his', 'canon', 'gainst', 'selfslaughter'], ['o', 'god'], ['god'], ['how', 'weary', 'stale', 'flat', 'and', 'unprofitable', 'seem', 'to', 'me', 'all', 'the', 'uses', 'of', 'this', 'world'], ['fie', 'ont'], ['ah', 'fie'], ['tis', 'an', 'unweeded', 'garden', 'that', 'grows', 'to', 'seed', 'things', 'rank', 'and', 'gross', 'in', 'nature', 'possess', 'it', 'merely'], ['that', 'it', 'should', 'come', 'to', 'this'], ['but', 'two', 'months', 'dead'], ['nay', 'not', 'so', 'much', 'not', 'two'], ['so', 'excellent', 'a', 'king', 'that', 'was', 'to', 'this', 'hyperion', 'to', 'a', 'satyr', 'so', 'loving', 'to', 'my', 'mother', 'that', 'he', 'might', 'not', 'beteem', 'the', 'winds', 'of', 'heaven', 'visit', 'her', 'face', 'too', 'roughly'], ['heaven', 'and', 'earth'], ['must', 'i', 'remember'], ['why', 'she', 'would', 'hang', 'on', 'him', 'as', 'if', 'increase', 'of', 'appetite', 'had', 'grown', 'by', 'what', 'it', 'fed', 'on', 'and', 'yet', 'within', 'a', 'month', 'let', 'me', 'not', 'think', 'ont'], ['frailty', 'thy', 'name', 'is', 'woman'], ['a', 'little', 'month', 'or', 'ere', 'those', 'shoes', 'were', 'old', 'with', 'which', 'she', 'followed', 'my', 'poor', 'fathers', 'body', 'like', 'niobe', 'all', 'tears', 'why', 'she', 'even', 'she', 'o', 'god'], ['a', 'beast', 'that', 'wants', 'discourse', 'of', 'reason', 'would', 'have', 'mournd', 'longer', 'married', 'with', 'my', 'uncle', 'my', 'fathers', 'brother', 'but', 'no', 'more', 'like', 'my', 'father', 'than', 'i', 'to', 'hercules'], ['within', 'a', 'month', 'ere', 'yet', 'the', 'salt', 'of', 'most', 'unrighteous', 'tears', 'had', 'left', 'the', 'flushing', 'in', 'her', 'galled', 'eyes', 'she', 'married'], ['o', 'most', 'wicked', 'speed', 'to', 'post', 'with', 'such', 'dexterity', 'to', 'incestuous', 'sheets'], ['it', 'is', 'not', 'nor', 'it', 'cannot', 'come', 'to', 'good'], ['but', 'break', 'my', 'heart', 'for', 'i', 'must', 'hold', 'my', 'tongue']]\n"
     ]
    }
   ],
   "source": [
    "unknown = processGutenberg('testText.txt', 'Unknown', make=False)\n",
    "print(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 16187)\n",
      "('and', 12056)\n",
      "('*', 25974)\n",
      "25974\n"
     ]
    }
   ],
   "source": [
    "greatExp.wordCount\n",
    "count = 0\n",
    "for key, val in romeoJuliet.wordCount.iteritems():\n",
    "    if val > 10000:\n",
    "        print(key, val)\n",
    "        count += 1\n",
    "print(greatExp.wordCount['*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "def pickleDump(filename, todump):\n",
    "    out = open(filename, 'wb+')\n",
    "    for d in todump:\n",
    "        dump(d, out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickleDump('GreatExp.dat', [greatExp.chain, greatExp.wordCount, greatExp.author])\n",
    "pickleDump('Frankenstein.dat', [frank.chain, frank.wordCount, frank.author])\n",
    "pickleDump('RomeoJuliet.dat', [romeoJuliet.chain, romeoJuliet.wordCount, romeoJuliet.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickleDump('testText.dat', [unknown])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
