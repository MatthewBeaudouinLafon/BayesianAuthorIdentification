{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Generation\n",
    "In this notebook, we generate the necessary Markov Chains for each author which will be used as likelihood functions during the identification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import sys\n",
    "import re\n",
    "\n",
    "class MarkovChain():\n",
    "    \"\"\"\n",
    "    Markov Chain class. \n",
    "    chain:       nested dictionary representing the number of occurences of a word given the previous word.\n",
    "    wordCount:   dictionary of the number of total number of words (value) that have occured after the previous word (key).\n",
    "    \"\"\"\n",
    "    chain = {'*': {}}\n",
    "    wordCount = {'*': 0}\n",
    "    \n",
    "    def addWord(self, prevWord, word):\n",
    "        \"\"\"\n",
    "        Add a word to the Markov Chain.\n",
    "        Takes the previous word and the current word as strings\n",
    "        \"\"\"\n",
    "        self.chain[prevWord][word] = 1 + self.chain[prevWord].get(word, 0)\n",
    "        self.wordCount[prevWord]= 1 + self.wordCount.get(prevWord, 0)\n",
    "    \n",
    "        # When encountering a new word, add it to the prefix dictionary\n",
    "        if not self.chain.get(word):\n",
    "            self.chain[word] = {}\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Process a \"sentence\" to produce a Markov Chain. Takes a sentence as a list of lowercase words.\n",
    "        NO ASTERISKS. SERIOUSLY.\n",
    "        \"\"\"\n",
    "        # '*' represents the beginning of a sentence in the chain. \n",
    "        # This way, we can determine the probability of a word starting a sentence\n",
    "        sentence = ['*'] + sentence\n",
    "        if len(sentence) > 1:\n",
    "            for i in [i + 1 for i in range(len(sentence)-1)]: # Start at the second word\n",
    "                self.addWord(sentence[i-1], sentence[i])\n",
    "        \n",
    "    def getProb(self, prevWord, word):\n",
    "        \"\"\"\n",
    "        Return the probability of getting word given prevWord.\n",
    "        Takes two strings.\n",
    "        \"\"\"\n",
    "        return self.chain[prevWord][word]/self.wordCount[prevWord]\n",
    "    \n",
    "def processGutenberg(fileName):\n",
    "    \"\"\"\n",
    "    Process a Gutenberg text file.\n",
    "    fileName: string\n",
    "    returns a markovChain object.\n",
    "    \"\"\"\n",
    "    f = open(fileName)\n",
    "    \n",
    "    #Skip to the beginning of the actual text\n",
    "    for line in f:\n",
    "        if line.startswith(\"*** START OF THIS PROJECT\"):\n",
    "            break\n",
    "    \n",
    "    text = ''\n",
    "    markovChain = MarkovChain()\n",
    "    \n",
    "    # Put the text into one big string\n",
    "    for line in f:\n",
    "        # Stop when hitting the end of the book\n",
    "        if line.startswith(\"*** END OF THIS PROJECT\"):\n",
    "            break\n",
    "        \n",
    "        text += line + ' '\n",
    "        \n",
    "    sentences = re.split('[.?!]', text) # Seperate text into a list of sentences sentence\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Make all words lowercase and strip off punctuation\n",
    "        sentenceList = ''.join(char for char in sentence if char in set(string.letters + string.digits + ' ')).lower().split()\n",
    "        \n",
    "        # Process the sentence in the Markov Chain\n",
    "        markovChain.addSentence(sentenceList)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return markovChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "greatExp = processGutenberg('GreatExpectations.txt')\n",
    "frank = processGutenberg('Frankenstein.txt')\n",
    "romeoJuliet = processGutenberg('RomeoAndJuliet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('to', 13733)\n",
      "('the', 24310)\n",
      "('a', 10739)\n",
      "('i', 16298)\n",
      "('and', 19013)\n",
      "('of', 13689)\n",
      "('*', 36692)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "romeoJuliet.wordCount\n",
    "count = 0\n",
    "for key, val in romeoJuliet.wordCount.iteritems():\n",
    "    if val > 10000:\n",
    "        print(key, val)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "def pickleDump(filename, todump):\n",
    "    out = open(filename, 'wb+')\n",
    "    for d in todump:\n",
    "        dump(d, out)\n",
    "    out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickleDump('GreatExp.dat', [greatExp.chain, greatExp.wordCount])\n",
    "pickleDump('Frankenstein.dat', [frank.chain, frank.wordCount])\n",
    "pickleDump('RomeoJuliet.dat', [romeoJuliet.chain, romeoJuliet.wordCount])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
