{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Generation\n",
    "In this notebook, we generate the necessary Markov Chains for each author which will be used as likelihood functions during the identification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import sys\n",
    "import re\n",
    "from pickle import dump\n",
    "\n",
    "def pickleDump(filename, todump):\n",
    "    out = open(filename, 'wb+')\n",
    "    for d in todump:\n",
    "        dump(d, out)\n",
    "    out.close()\n",
    "    \n",
    "class MarkovChain():\n",
    "    \"\"\"\n",
    "    Markov Chain class. \n",
    "    chain:       nested dictionary representing the number of occurences of a word given the previous word.\n",
    "    wordCount:   dictionary of the number of total number of words (value) that have occured after the previous word (key).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, author):\n",
    "        \"\"\"\n",
    "        Parameter: \n",
    "            - author: name of author (string)\n",
    "        \"\"\"\n",
    "        self.author = author\n",
    "        self.chain = {'*': {}}\n",
    "        self.wordCount = {'*': 0}\n",
    "    \n",
    "    def addWord(self, prevWord, word):\n",
    "        \"\"\"\n",
    "        Add a word to the Markov Chain.\n",
    "        Takes the previous word and the current word as strings\n",
    "        \"\"\"\n",
    "        self.chain[prevWord][word] = 1 + self.chain[prevWord].get(word, 0)\n",
    "        self.wordCount[prevWord]= 1 + self.wordCount.get(prevWord, 0)\n",
    "    \n",
    "        # When encountering a new word, add it to the prefix dictionary\n",
    "        if not self.chain.get(word):\n",
    "            self.chain[word] = {}\n",
    "        \n",
    "    def addSentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Process a \"sentence\" to produce a Markov Chain. Takes a sentence as a list of lowercase words.\n",
    "        NO ASTERISKS. SERIOUSLY.\n",
    "        \"\"\"\n",
    "        # '*' represents the beginning of a sentence in the chain. \n",
    "        # This way, we can determine the probability of a word starting a sentence\n",
    "        sentence = ['*'] + sentence\n",
    "        if len(sentence) > 1:\n",
    "            for i in [i + 1 for i in range(len(sentence)-1)]: # Start at the second word\n",
    "                self.addWord(sentence[i-1], sentence[i])\n",
    "        \n",
    "    def getProb(self, prevWord, word):\n",
    "        \"\"\"\n",
    "        Return the probability of getting word given prevWord.\n",
    "        Takes two strings.\n",
    "        \"\"\"\n",
    "        return self.chain[prevWord][word]/self.wordCount[prevWord]\n",
    "    \n",
    "def processGutenberg(fileName, author, make=True):\n",
    "    \"\"\"\n",
    "    Process a Gutenberg text file.\n",
    "    fileName: string\n",
    "    author: string\n",
    "    returns a markovChain object.\n",
    "    \"\"\"\n",
    "    f = open(fileName)\n",
    "    \n",
    "    #Skip to the beginning of the actual text\n",
    "    for line in f:\n",
    "        if line.startswith(\"*** START OF THIS PROJECT\"):\n",
    "            break\n",
    "    \n",
    "    text = ''\n",
    "    \n",
    "    # Put the text into one big string\n",
    "    for line in f:\n",
    "        # Stop when hitting the end of the book\n",
    "        if line.startswith(\"*** END OF THIS PROJECT\"):\n",
    "            break\n",
    "        \n",
    "        text += line + ' '\n",
    "        \n",
    "    sentences = re.split('[.?!]', text) # Seperate text into a list of sentences sentence\n",
    "    \n",
    "    listOSentences = []\n",
    "    for sentence in sentences:\n",
    "        # Make all words lowercase and strip off punctuation\n",
    "        sentenceList = ''.join(char for char in sentence if char in set(string.letters + string.digits + ' ')).lower().split()\n",
    "        \n",
    "        if sentenceList != []:\n",
    "            listOSentences.append(sentenceList)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    if make==True:\n",
    "        return makeMarkov(listOSentences, author)\n",
    "    else:\n",
    "        return listOSentences\n",
    "\n",
    "def makeMarkov(sentenceList, author):\n",
    "    markovChain = MarkovChain(author)\n",
    "    for sentence in sentenceList:\n",
    "        # Process the sentence in the Markov Chain\n",
    "        markovChain.addSentence(sentence)\n",
    "    return markovChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "greatExp = processGutenberg('GreatExpectations.txt', 'Charles Dickens')\n",
    "frank = processGutenberg('Frankenstein.txt', 'Mary Shelley')\n",
    "romeoJuliet = processGutenberg('RomeoAndJuliet.txt', 'Shakespeare')\n",
    "\n",
    "pickleDump('GreatExp.dat', [greatExp.chain, greatExp.wordCount, greatExp.author])\n",
    "pickleDump('Frankenstein.dat', [frank.chain, frank.wordCount, frank.author])\n",
    "pickleDump('RomeoJuliet.dat', [romeoJuliet.chain, romeoJuliet.wordCount, romeoJuliet.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update test Text\n",
    "unknown = processGutenberg('testText.txt', 'Unknown', make=False)\n",
    "pickleDump('testText.dat', [unknown])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 16187)\n",
      "('and', 12056)\n",
      "('*', 25974)\n",
      "25974\n"
     ]
    }
   ],
   "source": [
    "greatExp.wordCount\n",
    "count = 0\n",
    "for key, val in romeoJuliet.wordCount.iteritems():\n",
    "    if val > 10000:\n",
    "        print(key, val)\n",
    "        count += 1\n",
    "print(greatExp.wordCount['*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
